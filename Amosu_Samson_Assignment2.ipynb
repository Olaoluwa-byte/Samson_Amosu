{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including pandas, numpy, sklearn, gensim, and wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade scipy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore Dataset\n",
    "Load the IMDB_Reviews.csv dataset and explore its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Explore Dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('IMDB_Reviews.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Display the structure of the dataset\n",
    "df.info()\n",
    "\n",
    "# Display basic statistics of the dataset\n",
    "df.describe()\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Display the distribution of sentiments\n",
    "df['sentiment'].value_counts()\n",
    "\n",
    "# Randomly sample 5,000 reviews for the assignment tasks\n",
    "df_sample = df.sample(n=5000, random_state=42)\n",
    "\n",
    "# Display the first few rows of the sampled dataset\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling of Data\n",
    "Select a random sample of 5,000 movie reviews from the dataset for the assignment tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5,000 reviews for the assignment tasks\n",
    "df_sample = df.sample(n=5000, random_state=42)\n",
    "\n",
    "# Display the first few rows of the sampled dataset\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "Preprocess the text data by removing stop words, punctuation, and performing tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words, lemmatizer, and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stop words and punctuation, and perform lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in punctuation]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the review column\n",
    "df_sample['cleaned_review'] = df_sample['review'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Topic Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LDA Model\n",
    "Train an LDA model with at least 10 topics using the preprocessed text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Topic Modeling\n",
    "\n",
    "# Create a dictionary and corpus for the LDA model\n",
    "dictionary = corpora.Dictionary(df_sample['cleaned_review'].apply(lambda x: x.split()))\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df_sample['cleaned_review']]\n",
    "\n",
    "# Train the LDA model with at least 10 topics\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "for num_topics in topics:\n",
    "    print(num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Topics and Keywords\n",
    "Print out the list of topics and at least 20 keywords for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the list of topics and at least 20 keywords for each topic\n",
    "topics = lda_model.print_topics(num_words=20)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Topics with Word Cloud\n",
    "Pick at least 5 topics from the LDA model and visualize each topic with a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick at least 5 topics from the LDA model and visualize each topic with word cloud\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(dict(lda_model.show_topic(i, 20)))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f'Topic {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sentiment Analysis using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Sentiment Analysis using TF-IDF\n",
    "\n",
    "# Preprocess the text before building TF-IDF representation\n",
    "df_sample['cleaned_review'] = df_sample['review'].apply(preprocess_text)\n",
    "\n",
    "# Build TF-IDF representation from the review text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(df_sample['cleaned_review'])\n",
    "\n",
    "# Split the dataset into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a machine learning model (Naive Bayes) on the training set to predict sentiment of review text\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model and print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF-IDF Representation\n",
    "Build a TF-IDF representation from the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Sentiment Analysis using TF-IDF\n",
    "\n",
    "# Preprocess the text before building TF-IDF representation\n",
    "df_sample['cleaned_review'] = df_sample['review'].apply(preprocess_text)\n",
    "\n",
    "# Build TF-IDF representation from the review text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(df_sample['cleaned_review'])\n",
    "\n",
    "# Split the dataset into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a machine learning model (Naive Bayes) on the training set to predict sentiment of review text\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model and print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Testing Sets\n",
    "Split the data into training and testing sets with an 80%/20% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_sample['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Machine Learning Model\n",
    "Train a machine learning model on the training set to predict the sentiment of review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a machine learning model (Naive Bayes) on the training set to predict sentiment of review text\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model and print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Make predictions and evaluate the model on the testing set. Print the classification report to show the prediction metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model and print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
